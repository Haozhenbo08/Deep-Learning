{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw2_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtcBjMq7YV3f"
      },
      "source": [
        "\n",
        "\n",
        "# Homework 2 - Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn-cOk1iZTtR"
      },
      "source": [
        "In this part of the homework we are going to work with Recurrent Neural Networks, in particular GRU. One of the greatest things that Recurrent Neural Networks can do when working with sequences is retaining data from several timesteps in the past. We are going to explore that property by constructing an 'echo' Recurrent Neural Network.\n",
        "\n",
        "The goal here is to make a model that given a sequence of letters or digits will output that same sequence, but with a certain delay. Let's say the input is a string 'abacaba', we want the model to not output anything for 3 steps (delay length), and then output the original string step by step, except the last 3 characters. So, target output is then 'XXXabac', where 'X' is empty output.\n",
        "\n",
        "This is similar to [this notebook](https://github.com/Atcold/pytorch-Deep-Learning/blob/master/09-echo_data.ipynb) (which you should refer to when doing this assignment), except we're working not with a binary string, but with a sequence of integers between 0 and some N. In our case N is 26, which is the number of letters in the alphabet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npLlE973as6x"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Let's implement the dataset. In our case, the data is basically infinite, as we can always generate more examples on the fly, so there's no need to load it from disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkEEMyvzIMRx"
      },
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "import torch\n",
        "\n",
        "# Max value of the generated integer. 26 is chosen becuase it's\n",
        "# the number of letters in English alphabet.\n",
        "N = 26\n",
        "\n",
        "\n",
        "def idx_to_onehot(x, k=N+1):\n",
        "  \"\"\" Converts the generated integers to one-hot vectors \"\"\"\n",
        "  ones = torch.sparse.torch.eye(k)\n",
        "  shape = x.shape\n",
        "  res = ones.index_select(0, x.view(-1).type(torch.int64))\n",
        "  return res.view(*shape, res.shape[-1])\n",
        "\n",
        "def str_to_idx(s): \n",
        "  n = []\n",
        "  for x in s:\n",
        "    n.append(ord(x) - 96)\n",
        "  return torch.tensor(n)\n",
        "\n",
        "def out_to_str(o):\n",
        "  idx = torch.argmax(o, dim=-1).view(o.shape[1])\n",
        "  s = ''\n",
        "  for i in idx:\n",
        "    if i!=0:\n",
        "      s+=(chr(i+96))\n",
        "    else:\n",
        "      s+=(' ')\n",
        "  return s\n",
        "\n",
        "class EchoDataset(torch.utils.data.IterableDataset):\n",
        "\n",
        "  def __init__(self, delay=4, seq_length=15, size=1000):\n",
        "    self.delay = delay\n",
        "    self.seq_length = seq_length\n",
        "    self.size = size\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.size\n",
        "\n",
        "  def __iter__(self):\n",
        "    \"\"\" Iterable dataset doesn't have to implement __getitem__.\n",
        "        Instead, we only need to implement __iter__ to return\n",
        "        an iterator (or generator).\n",
        "    \"\"\"\n",
        "    for _ in range(self.size):\n",
        "      seq = torch.tensor([random.choice(range(1, N + 1)) for i in range(self.seq_length)], dtype=torch.int64)\n",
        "      result = torch.cat((torch.zeros(self.delay), seq[:self.seq_length - self.delay])).type(torch.int64)\n",
        "      yield seq, result\n",
        "\n",
        "DELAY = 4\n",
        "DATASET_SIZE = 200000\n",
        "ds = EchoDataset(delay=DELAY, size=DATASET_SIZE)\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Fl4AB0rRFyR9"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNrZqYURcKSl"
      },
      "source": [
        "## Model\n",
        "\n",
        "Now, we want to implement the model. For our purposes, we want to use GRU. The architecture consists of GRU and a decoder. Decoder is responsible for decoding the GRU hidden state to yield a predicting for the next output. The parts you are responsible for filling with your code are marked with `TODO`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nigN_o4Mb9Nx"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class GRUMemory(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size):\n",
        "    super().__init__()\n",
        "    #TODO: initialize your submodules\n",
        "    self.hidden_size = hidden_size\n",
        "    self.gru = torch.nn.GRU(27, self.hidden_size, num_layers=2, batch_first=True)\n",
        "    self.fc1 = torch.nn.Linear(in_features=self.hidden_size, out_features=27)\n",
        "  def forward(self, x):\n",
        "    # inputs: x - input tensor of shape (batch_size, seq_length, N+1)\n",
        "    # returns:\n",
        "    # logits (scores for softmax) of shape (batch size, seq_length, N + 1)\n",
        "    # TODO implement forward pass\n",
        "    hidden_state = torch.zeros(2, x.size(0), self.hidden_size).to(device)\n",
        "    output, hidden_state = self.gru(x, hidden_state)\n",
        "    output = self.fc1(output)\n",
        "    return output,hidden_state\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def test_run(self, s):\n",
        "    # This function accepts one string s containing lowercase characters a-z. \n",
        "    # You need to map those characters to one-hot encodings, \n",
        "    # then get the result from your network, and then convert the output \n",
        "    # back to a string of the same length, with 0 mapped to ' ', \n",
        "    # and 1-26 mapped to a-z.\n",
        "\n",
        "    # TODO\n",
        "    idx = str_to_idx(s)\n",
        "    input = idx_to_onehot(idx).view(-1,len(s),27).to(device)\n",
        "    output,_ = self.forward(input)\n",
        "    #print(output,output.shape,output.sum(-1),output.sum(-1).shape)\n",
        "    output_str = out_to_str(output)\n",
        "    return output_str"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9whwmVu9OIx"
      },
      "source": [
        "## Training\n",
        "Below you need to implement the training of the model. We give you more freedom as for the implementation. The two limitations are that it has to execute within 10 minutes, and that error rate should be below 1%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUZkeRnVTNzG"
      },
      "source": [
        "def test_model(model, sequence_length=15):\n",
        "  \"\"\"\n",
        "  This is the test function that runs 100 different strings through your model,\n",
        "  and checks the error rate.\n",
        "  \"\"\"\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  for i in range(100):\n",
        "    s = ''.join([random.choice(string.ascii_lowercase) for i in range(random.randint(15, 25))])\n",
        "    result = model.test_run(s)\n",
        "    for c1, c2 in zip(s[:-DELAY], result[DELAY:]):\n",
        "      correct += int(c1 == c2)\n",
        "    total += len(s) - DELAY\n",
        "\n",
        "  return correct / total"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lV9BscxCCAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cdbd343-a923-4b2d-dd53-7a81776eee87"
      },
      "source": [
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "start_time = time.time()\n",
        "\n",
        "# TODO\n",
        "model = GRUMemory(18).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "batch_size = 128\n",
        "Epochs = 3\n",
        "train_dataloader = DataLoader(ds, batch_size)\n",
        "for i in range(Epochs):\n",
        "  print(i)\n",
        "  for x,y in train_dataloader:\n",
        "    x = idx_to_onehot(x)\n",
        "    y = idx_to_onehot(y)\n",
        "    # x 64*15*27 y 64*15\n",
        "    x,y = x.to(device),y.to(device)\n",
        "    logits, _ = model(x)\n",
        "    loss = criterion(logits[:,4:15,:], y[:,4:15,:])\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(test_model(model),loss,time.time()-start_time) \n",
        "    \n",
        "\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "accuracy = test_model(model)\n",
        "assert duration < 600, 'execution took f{duration:.2f} seconds, which longer than 10 mins'\n",
        "assert accuracy > 0.99, f'accuracy is too low, got {accuracy}, need 0.99'\n",
        "print('tests passed')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0.941983780411728 tensor(0.1667, device='cuda:0', grad_fn=<DivBackward1>) 13.169416427612305\n",
            "1\n",
            "0.9861198738170347 tensor(0.1185, device='cuda:0', grad_fn=<DivBackward1>) 26.200608253479004\n",
            "2\n",
            "0.9993714644877436 tensor(0.1013, device='cuda:0', grad_fn=<DivBackward1>) 39.10888671875\n",
            "tests passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8owwdzBJRdur",
        "outputId": "2d486661-d338-4351-89aa-e400b0beefd7"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB0EVNBtDhpN"
      },
      "source": [
        "## Variable delay model\n",
        "\n",
        "Now, to make this more complicated, we want to have varialbe delay. So, now, the goal is to transform a sequence of pairs (character, delay) into a character sequence with given delay. Delay is constant within one sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i_iwX_AEOCH"
      },
      "source": [
        "### Dataset\n",
        "As before, we first implement the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4G5b8kuEUEd"
      },
      "source": [
        "\n",
        "import torch\n",
        "import random\n",
        "class VariableDelayEchoDataset(torch.utils.data.IterableDataset):\n",
        "\n",
        "  def __init__(self, max_delay=8, seq_length=20, size=2000):\n",
        "    self.max_delay = max_delay\n",
        "    self.seq_length = seq_length\n",
        "    self.size = size\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.size\n",
        "\n",
        "  def __iter__(self):\n",
        "    for _ in range(self.size):\n",
        "      seq = torch.tensor([random.choice(range(1, N + 1)) for i in range(self.seq_length)], dtype=torch.int64)\n",
        "      delay = random.randint(0, self.max_delay)\n",
        "      result = torch.cat((torch.zeros(delay), seq[:self.seq_length - delay])).type(torch.int64)\n",
        "      yield seq, delay, result\n",
        "\n",
        "vds = VariableDelayEchoDataset(max_delay=8, seq_length=20)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTRVOND3HEJZ"
      },
      "source": [
        "### Model\n",
        "\n",
        "And the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYolFIB8Hg0U"
      },
      "source": [
        "class VariableDelayGRUMemory(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, max_delay):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.max_delay = max_delay\n",
        "    self.gru = torch.nn.GRU(27, self.hidden_size, num_layers=8, batch_first=True)\n",
        "    self.fc1 = torch.nn.Linear(in_features=self.hidden_size, out_features=27)\n",
        "\n",
        "  def forward(self, x, delays):\n",
        "    # inputs:\n",
        "    # x - tensor of shape (batch size, seq length, N + 1)\n",
        "    # delays - tensor of shape (batch size)\n",
        "    # returns:\n",
        "    # logits (scores for softmax) of shape (batch size, seq_length, N + 1)\n",
        "    hidden_state = delays.unsqueeze(-1).repeat(8,1,hidden_size).float().to(device)\n",
        "    output, hidden_state = self.gru(x, hidden_state)\n",
        "    output = self.fc1(output)\n",
        "    return output,hidden_state\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def test_run(self, s, delay):\n",
        "    # This function accepts one string s containing lowercase characters a-z, \n",
        "    # and a delay - the desired output delay.\n",
        "    # You need to map those characters to one-hot encodings, \n",
        "    # then get the result from your network, and then convert the output \n",
        "    # back to a string of the same length, with 0 mapped to ' ', \n",
        "    # and 1-26 mapped to a-z.\n",
        "\n",
        "    idx = str_to_idx(s)\n",
        "    input = idx_to_onehot(idx).view(-1,len(s),27).to(device)\n",
        "    output,_ = self.forward(input,torch.tensor(delay))\n",
        "    #print(output,output.shape,output.sum(-1),output.sum(-1).shape)\n",
        "    output_str = out_to_str(output)\n",
        "    return output_str\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riu3qHWgKjsx"
      },
      "source": [
        "### Train\n",
        "\n",
        "As before, you're free to do what you want, as long as training finishes within 10 minutes and accuracy is above 0.99 for delays between 0 and 8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FZHojnGO3aw"
      },
      "source": [
        "def test_variable_delay_model(model, seq_length=20):\n",
        "  \"\"\"\n",
        "  This is the test function that runs 100 different strings through your model,\n",
        "  and checks the error rate.\n",
        "  \"\"\"\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  for i in range(100):\n",
        "    s = ''.join([random.choice(string.ascii_lowercase) for i in range(seq_length)])\n",
        "    d = random.randint(0, model.max_delay)\n",
        "    result = model.test_run(s, d)\n",
        "    #print(s,d,result)\n",
        "    if d > 0:\n",
        "      z = zip(s[:-d], result[d:])\n",
        "    else:\n",
        "      z = zip(s, result)\n",
        "    for c1, c2 in z:\n",
        "      correct += int(c1 == c2)\n",
        "    total += len(s) - d\n",
        "\n",
        "  return correct / total"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ18Ef6vKi4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58ab1613-f823-4d80-997a-d98cacd57e0d"
      },
      "source": [
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "start_time = time.time()\n",
        "\n",
        "MAX_DELAY = 8\n",
        "SEQ_LENGTH = 20\n",
        "\n",
        "# TODO\n",
        "hidden_size = 18\n",
        "model = VariableDelayGRUMemory(hidden_size,MAX_DELAY).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "batch_size = 64\n",
        "Epochs = 100\n",
        "train_dataloader = DataLoader(vds, batch_size)\n",
        "for i in range(Epochs):\n",
        "  print(i)\n",
        "  for x,d,y in train_dataloader:\n",
        "    x = idx_to_onehot(x)\n",
        "    #y = idx_to_onehot(y)\n",
        "    # logits batchsize*15*27 y batchsize*15\n",
        "    x,d,y = x.to(device),d.to(device),y.to(device)\n",
        "    logits, _ = model(x,d)\n",
        "    loss = 0\n",
        "    for j in range(d.shape[0]):\n",
        "      a = logits[j,d[j]:SEQ_LENGTH,:]\n",
        "      a.view(27,SEQ_LENGTH-d[j])\n",
        "      b = y[j,d[j]:SEQ_LENGTH]\n",
        "      b.view(SEQ_LENGTH-d[j])\n",
        "      loss+=criterion(a,b)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(test_variable_delay_model(model),loss,time.time()-start_time) \n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "assert end_time - start_time < 600, 'executing took longer than 10 mins'\n",
        "assert test_variable_delay_model(model) > 0.99, 'accuracy is too low'\n",
        "print('tests passed')\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0.03927680798004988 tensor(52.1379, device='cuda:0', grad_fn=<AddBackward0>) 1.2493586540222168\n",
            "1\n",
            "0.048994974874371856 tensor(51.7949, device='cuda:0', grad_fn=<AddBackward0>) 2.465651035308838\n",
            "2\n",
            "0.07349246231155779 tensor(50.7147, device='cuda:0', grad_fn=<AddBackward0>) 3.719346046447754\n",
            "3\n",
            "0.136789506558401 tensor(47.4815, device='cuda:0', grad_fn=<AddBackward0>) 4.964017629623413\n",
            "4\n",
            "0.1334966319657073 tensor(44.9203, device='cuda:0', grad_fn=<AddBackward0>) 6.196865797042847\n",
            "5\n",
            "0.20649509803921567 tensor(39.8775, device='cuda:0', grad_fn=<AddBackward0>) 7.415040016174316\n",
            "6\n",
            "0.2201776649746193 tensor(34.9235, device='cuda:0', grad_fn=<AddBackward0>) 8.648558616638184\n",
            "7\n",
            "0.27467001885606535 tensor(33.8677, device='cuda:0', grad_fn=<AddBackward0>) 9.85156536102295\n",
            "8\n",
            "0.2808988764044944 tensor(32.5781, device='cuda:0', grad_fn=<AddBackward0>) 11.06050968170166\n",
            "9\n",
            "0.49906890130353815 tensor(23.4305, device='cuda:0', grad_fn=<AddBackward0>) 12.290926694869995\n",
            "10\n",
            "0.3807073954983923 tensor(28.6409, device='cuda:0', grad_fn=<AddBackward0>) 13.518140077590942\n",
            "11\n",
            "0.5164290142591444 tensor(20.0770, device='cuda:0', grad_fn=<AddBackward0>) 14.745007514953613\n",
            "12\n",
            "0.6350093109869647 tensor(15.1383, device='cuda:0', grad_fn=<AddBackward0>) 15.955310583114624\n",
            "13\n",
            "0.6246076585059636 tensor(11.9703, device='cuda:0', grad_fn=<AddBackward0>) 17.178716897964478\n",
            "14\n",
            "0.4638052530429212 tensor(27.3957, device='cuda:0', grad_fn=<AddBackward0>) 18.372503995895386\n",
            "15\n",
            "0.6649282920469362 tensor(16.7782, device='cuda:0', grad_fn=<AddBackward0>) 19.59825325012207\n",
            "16\n",
            "0.6832377310388783 tensor(13.4875, device='cuda:0', grad_fn=<AddBackward0>) 20.808271646499634\n",
            "17\n",
            "0.6454033771106942 tensor(19.1445, device='cuda:0', grad_fn=<AddBackward0>) 22.02524209022522\n",
            "18\n",
            "0.7632575757575758 tensor(12.5616, device='cuda:0', grad_fn=<AddBackward0>) 23.242928981781006\n",
            "19\n",
            "0.7375707992448081 tensor(19.2759, device='cuda:0', grad_fn=<AddBackward0>) 24.49490451812744\n",
            "20\n",
            "0.7946484131922837 tensor(10.8160, device='cuda:0', grad_fn=<AddBackward0>) 25.717692375183105\n",
            "21\n",
            "0.7264325323475046 tensor(13.2031, device='cuda:0', grad_fn=<AddBackward0>) 26.93545150756836\n",
            "22\n",
            "0.7178030303030303 tensor(17.2776, device='cuda:0', grad_fn=<AddBackward0>) 28.13113832473755\n",
            "23\n",
            "0.8113440197287299 tensor(5.8469, device='cuda:0', grad_fn=<AddBackward0>) 29.335580825805664\n",
            "24\n",
            "0.7260621433100825 tensor(6.8950, device='cuda:0', grad_fn=<AddBackward0>) 30.532018184661865\n",
            "25\n",
            "0.6852078239608802 tensor(13.2270, device='cuda:0', grad_fn=<AddBackward0>) 31.75197196006775\n",
            "26\n",
            "0.764404609475032 tensor(10.2571, device='cuda:0', grad_fn=<AddBackward0>) 32.97043704986572\n",
            "27\n",
            "0.8641819330385344 tensor(5.8010, device='cuda:0', grad_fn=<AddBackward0>) 34.19373083114624\n",
            "28\n",
            "0.9141257000622277 tensor(5.5658, device='cuda:0', grad_fn=<AddBackward0>) 35.43327832221985\n",
            "29\n",
            "0.8897882938978829 tensor(7.1209, device='cuda:0', grad_fn=<AddBackward0>) 36.62608194351196\n",
            "30\n",
            "0.8563569682151589 tensor(3.1454, device='cuda:0', grad_fn=<AddBackward0>) 37.84756064414978\n",
            "31\n",
            "0.761993769470405 tensor(7.7733, device='cuda:0', grad_fn=<AddBackward0>) 39.05731391906738\n",
            "32\n",
            "0.8579691516709511 tensor(8.7978, device='cuda:0', grad_fn=<AddBackward0>) 40.27617120742798\n",
            "33\n",
            "0.8653610771113831 tensor(6.6023, device='cuda:0', grad_fn=<AddBackward0>) 41.481905460357666\n",
            "34\n",
            "0.7775689223057645 tensor(7.3864, device='cuda:0', grad_fn=<AddBackward0>) 42.71293783187866\n",
            "35\n",
            "0.852195423623995 tensor(6.7052, device='cuda:0', grad_fn=<AddBackward0>) 43.91693830490112\n",
            "36\n",
            "0.8091259640102828 tensor(6.8565, device='cuda:0', grad_fn=<AddBackward0>) 45.11790347099304\n",
            "37\n",
            "0.9048223350253807 tensor(4.4029, device='cuda:0', grad_fn=<AddBackward0>) 46.34303140640259\n",
            "38\n",
            "0.9195183776932826 tensor(4.4099, device='cuda:0', grad_fn=<AddBackward0>) 47.554805755615234\n",
            "39\n",
            "0.9305019305019305 tensor(7.6285, device='cuda:0', grad_fn=<AddBackward0>) 48.76024675369263\n",
            "40\n",
            "0.9394904458598726 tensor(1.5166, device='cuda:0', grad_fn=<AddBackward0>) 49.96490812301636\n",
            "41\n",
            "0.862708719851577 tensor(4.2566, device='cuda:0', grad_fn=<AddBackward0>) 51.1817946434021\n",
            "42\n",
            "0.901659496004917 tensor(1.6005, device='cuda:0', grad_fn=<AddBackward0>) 52.403719663619995\n",
            "43\n",
            "0.9471064094586186 tensor(2.9665, device='cuda:0', grad_fn=<AddBackward0>) 53.603107213974\n",
            "44\n",
            "0.9579349904397706 tensor(2.1656, device='cuda:0', grad_fn=<AddBackward0>) 54.814849615097046\n",
            "45\n",
            "0.9373040752351097 tensor(1.0842, device='cuda:0', grad_fn=<AddBackward0>) 56.07574939727783\n",
            "46\n",
            "0.7058096415327565 tensor(13.0647, device='cuda:0', grad_fn=<AddBackward0>) 57.303823709487915\n",
            "47\n",
            "0.9434782608695652 tensor(5.7673, device='cuda:0', grad_fn=<AddBackward0>) 58.50424075126648\n",
            "48\n",
            "0.986198243412798 tensor(1.4371, device='cuda:0', grad_fn=<AddBackward0>) 59.71907448768616\n",
            "49\n",
            "0.9889570552147239 tensor(1.2262, device='cuda:0', grad_fn=<AddBackward0>) 60.922197341918945\n",
            "50\n",
            "0.9987593052109182 tensor(0.5143, device='cuda:0', grad_fn=<AddBackward0>) 62.124422550201416\n",
            "51\n",
            "0.9964028776978417 tensor(0.4244, device='cuda:0', grad_fn=<AddBackward0>) 63.32670211791992\n",
            "52\n",
            "0.9981214777708203 tensor(0.4363, device='cuda:0', grad_fn=<AddBackward0>) 64.53363156318665\n",
            "53\n",
            "0.9554858934169279 tensor(0.3568, device='cuda:0', grad_fn=<AddBackward0>) 65.8023316860199\n",
            "54\n",
            "0.9245283018867925 tensor(4.3792, device='cuda:0', grad_fn=<AddBackward0>) 67.02218341827393\n",
            "55\n",
            "0.9622520793346129 tensor(0.5146, device='cuda:0', grad_fn=<AddBackward0>) 68.23065757751465\n",
            "56\n",
            "0.9905482041587902 tensor(0.5653, device='cuda:0', grad_fn=<AddBackward0>) 69.44099569320679\n",
            "57\n",
            "0.9993792675356921 tensor(0.1990, device='cuda:0', grad_fn=<AddBackward0>) 70.64352011680603\n",
            "58\n",
            "0.9975565058032987 tensor(0.5098, device='cuda:0', grad_fn=<AddBackward0>) 71.862389087677\n",
            "59\n",
            "0.9993548387096775 tensor(0.1862, device='cuda:0', grad_fn=<AddBackward0>) 73.06862545013428\n",
            "60\n",
            "0.9981179422835633 tensor(0.1931, device='cuda:0', grad_fn=<AddBackward0>) 74.25807857513428\n",
            "61\n",
            "0.9926650366748166 tensor(0.1501, device='cuda:0', grad_fn=<AddBackward0>) 75.45936822891235\n",
            "62\n",
            "0.9924146649810367 tensor(0.3471, device='cuda:0', grad_fn=<AddBackward0>) 76.70642566680908\n",
            "63\n",
            "0.6495297805642634 tensor(10.1219, device='cuda:0', grad_fn=<AddBackward0>) 77.93723225593567\n",
            "64\n",
            "0.5571428571428572 tensor(22.1684, device='cuda:0', grad_fn=<AddBackward0>) 79.12524223327637\n",
            "65\n",
            "0.8285009253547193 tensor(5.2276, device='cuda:0', grad_fn=<AddBackward0>) 80.31257557868958\n",
            "66\n",
            "0.9279503105590062 tensor(6.2401, device='cuda:0', grad_fn=<AddBackward0>) 81.53216481208801\n",
            "67\n",
            "0.9354644149577804 tensor(2.6286, device='cuda:0', grad_fn=<AddBackward0>) 82.7394380569458\n",
            "68\n",
            "0.9739938080495356 tensor(1.3858, device='cuda:0', grad_fn=<AddBackward0>) 83.98410511016846\n",
            "69\n",
            "0.9802712700369913 tensor(0.9199, device='cuda:0', grad_fn=<AddBackward0>) 85.18125820159912\n",
            "70\n",
            "0.9255784865540964 tensor(1.2260, device='cuda:0', grad_fn=<AddBackward0>) 86.41074299812317\n",
            "71\n",
            "0.9505012531328321 tensor(1.3677, device='cuda:0', grad_fn=<AddBackward0>) 87.63162469863892\n",
            "72\n",
            "0.9867591424968474 tensor(1.0734, device='cuda:0', grad_fn=<AddBackward0>) 88.84779047966003\n",
            "73\n",
            "0.996859296482412 tensor(0.7386, device='cuda:0', grad_fn=<AddBackward0>) 90.07158994674683\n",
            "74\n",
            "0.993073047858942 tensor(0.4142, device='cuda:0', grad_fn=<AddBackward0>) 91.35540533065796\n",
            "75\n",
            "0.9123809523809524 tensor(0.9799, device='cuda:0', grad_fn=<AddBackward0>) 92.59849953651428\n",
            "76\n",
            "0.9191358024691358 tensor(1.7255, device='cuda:0', grad_fn=<AddBackward0>) 93.79950332641602\n",
            "77\n",
            "0.9136645962732919 tensor(2.8270, device='cuda:0', grad_fn=<AddBackward0>) 95.00905871391296\n",
            "78\n",
            "0.9656441717791411 tensor(1.5942, device='cuda:0', grad_fn=<AddBackward0>) 96.21106338500977\n",
            "79\n",
            "0.9825109306683323 tensor(0.4637, device='cuda:0', grad_fn=<AddBackward0>) 97.41843104362488\n",
            "80\n",
            "0.9950217797137524 tensor(0.7381, device='cuda:0', grad_fn=<AddBackward0>) 98.62535214424133\n",
            "81\n",
            "1.0 tensor(0.2030, device='cuda:0', grad_fn=<AddBackward0>) 99.87185215950012\n",
            "82\n",
            "0.9993819530284301 tensor(0.1245, device='cuda:0', grad_fn=<AddBackward0>) 101.10000085830688\n",
            "83\n",
            "0.9993706733794839 tensor(0.1406, device='cuda:0', grad_fn=<AddBackward0>) 102.30908036231995\n",
            "84\n",
            "0.9993742177722152 tensor(0.2246, device='cuda:0', grad_fn=<AddBackward0>) 103.50389313697815\n",
            "85\n",
            "1.0 tensor(0.1104, device='cuda:0', grad_fn=<AddBackward0>) 104.72585988044739\n",
            "86\n",
            "1.0 tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>) 105.9276430606842\n",
            "87\n",
            "1.0 tensor(0.2770, device='cuda:0', grad_fn=<AddBackward0>) 107.14273023605347\n",
            "88\n",
            "1.0 tensor(0.0413, device='cuda:0', grad_fn=<AddBackward0>) 108.34963274002075\n",
            "89\n",
            "1.0 tensor(0.0671, device='cuda:0', grad_fn=<AddBackward0>) 109.55631470680237\n",
            "90\n",
            "0.9993642720915448 tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>) 110.76805019378662\n",
            "91\n",
            "0.9993618379068283 tensor(0.0555, device='cuda:0', grad_fn=<AddBackward0>) 112.00287079811096\n",
            "92\n",
            "1.0 tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>) 113.215829372406\n",
            "93\n",
            "0.999360204734485 tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>) 114.44278597831726\n",
            "94\n",
            "0.9987661937075879 tensor(0.2433, device='cuda:0', grad_fn=<AddBackward0>) 115.6423852443695\n",
            "95\n",
            "0.8771483131763208 tensor(1.1803, device='cuda:0', grad_fn=<AddBackward0>) 116.84116888046265\n",
            "96\n",
            "0.7634674922600619 tensor(5.0024, device='cuda:0', grad_fn=<AddBackward0>) 118.07575464248657\n",
            "97\n",
            "0.9638178415470992 tensor(0.8248, device='cuda:0', grad_fn=<AddBackward0>) 119.28087973594666\n",
            "98\n",
            "0.9950155763239875 tensor(0.2574, device='cuda:0', grad_fn=<AddBackward0>) 120.4832775592804\n",
            "99\n",
            "0.9987121699935608 tensor(0.1401, device='cuda:0', grad_fn=<AddBackward0>) 121.69059443473816\n",
            "tests passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_variable_delay_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd1WeAw8WtCL",
        "outputId": "6b819cc2-e544-4a96-c796-198b07f921f0"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fHtPwq2gm0Xu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}